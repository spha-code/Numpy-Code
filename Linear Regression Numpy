import numpy as np

#Linear Regression
#f = w * x

#declare two np arrays
#INPUT Array
X = np.array([1, 2, 3, 4], dtype=np.float32)
#OUTPUT Array
Y = np.array([3, 6, 9, 12], dtype=np.float32)
print(f"This is the array: {X}")
print(f"This is the array: {Y}")

w = 0.0

#model output f - forward pass

def forward(x):
  return w * x

#loss function loss = MSE Mean Squared Error

def loss(y, y_predicted):
  return ((y_predicted-y)**2).mean()

#gradient
# MSE Value = 1/N * (w*x-y)**2
#derivative of MSE Mean Squared Error dJ/dw = 1/N 2x (w*x-y)
def gradient(x, y, y_predicted):
  return np.dot(2*x, y_predicted-y).mean()

#Print Prediction before training f(5)
  print(f"This is the rate before training {forward(5)}")

#Below starts the training

learning_rate = 0.01
iterations = 10

for epochs in range(iterations):
  print(f"\nThis is epoch {epochs+1} of training")
  #prediction = forward pass


  print(f"Forward Values. The Target is {Y} : {w*X}")

  #loss
  l = loss(Y, w*X)
  print(f"The loss in epoch {epochs+1} is {l:.3f}")

  #gradients
  dw = gradient(X, Y, w*X)
  print(f"Gradient Derivative {dw:.2f}")

  #update weights
  w = w - learning_rate * dw
  print(f"Updated Weight: {w:.4f}")




